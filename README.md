# Pre-Trained-Backdoor-Attack

**Pre-Trained Backdoor Attack** is a research-grade implementation aimed at exploring a critical and emerging threat vector in modern machine learning pipelinesâ€”**backdoor attacks embedded in pre-trained encoders**. This project demonstrates how adversaries can inject stealthy triggers during the pre-training phase of an encoder model and then exploit these vulnerabilities in downstream tasks without altering the architecture or parameters of target models.

## ğŸŒ Overview

Pre-trained models have become a cornerstone of transfer learning and few-shot learning due to their versatility and performance. However, this paradigm shift also opens up novel attack surfaces. Unlike traditional backdoor attacks that target end-to-end classifiers, **this project focuses on compromising the encoder itself**, making it possible to affect *any* future model or task that uses the compromised encoder. This form of attack is stealthy, generalizable, and transferable.

The attack methodology leverages task-agnostic trigger embedding into the encoderâ€™s feature space during pre-training. Once embedded, the encoder behaves normally on clean data but exhibits malicious behavior when presented with specific trigger patternsâ€”effectively "flipping" class predictions or causing targeted misclassifications.

## ğŸ“ Repository Structure
```Pre-Trained-Backdoor-Attack/ 
â”‚ â”œâ”€â”€ ğŸ“‚ clip/
â”‚ â”œâ”€â”€ clip.py # CLIP model wrapper 
â”‚ â”œâ”€â”€ model.py # CLIP model architecture 
â”‚ â””â”€â”€ simple_tokenizer.py # Tokenizer for text input
â”‚ â”œâ”€â”€ ğŸ“‚ datasets/
â”‚ â”œâ”€â”€ backdoor_dataset.py # Base class for poisoned datasets 
â”‚ â”œâ”€â”€ cifar10_dataset.py # CIFAR-10 dataset handler 
â”‚ â”œâ”€â”€ gtsrb_dataset.py # GTSRB dataset handler 
â”‚ â”œâ”€â”€ stl10_dataset.py # STL10 dataset handler 
â”‚ â””â”€â”€ svhn_dataset.py # SVHN dataset handler 
â”‚ â”œâ”€â”€ ğŸ“‚ evaluation/
â”‚ â””â”€â”€ nn_classifier.py # Evaluation using nearest neighbor classifier 
â”‚ â”œâ”€â”€ ğŸ“‚ log/
â”‚ â””â”€â”€ [Dataset Logs] # Evaluation results for clean and backdoor settings 
â”‚ â”œâ”€â”€ ğŸ“‚ models/
â”‚ â””â”€â”€ [Trained Models] # Saved encoder and classifier models 
â”‚ â”œâ”€â”€ ğŸ“‚ references/
â”‚ â””â”€â”€ [Papers, Notes] # Related academic literature 
â”‚ â”œâ”€â”€ ğŸ“‚ scripts/
â”‚ â””â”€â”€ [Utilities] # Helper scripts for setup and processing 
â”‚ â”œâ”€â”€ ğŸ“‚ trigger/
â”‚ â””â”€â”€ [Trigger Files] # Backdoor trigger generator (e.g., patch/pattern-based) 
â”‚ â”œâ”€â”€ bdencoder.py # Backdoored encoder logic 
â”œâ”€â”€ pretraining_encoder.py # Encoder pre-training with backdoor injection 
â”œâ”€â”€ training_downstream_classifier.py # Train classifier on downstream task 
â”œâ”€â”€ zero_shot.py # Zero-shot evaluation of poisoned encoder 
â””â”€â”€ README.md # Project documentation
```


## ğŸ¯ Key Features

- âœ… **Encoder-Level Backdoor Injection**: Embed stealthy triggers during encoder pretraining.
- ğŸ” **Transferability**: Poisoned encoder affects multiple downstream tasks without retraining.
- ğŸ“¦ **Dataset Diversity**: Supports CIFAR-10, STL10, GTSRB, and SVHN for comparative evaluation.
- ğŸ§  **Zero-shot and Supervised Evaluation**: Validate both transfer learning and fully trained classifiers.
- ğŸ“Š **Log-supported Reproducibility**: All experimental outputs are preserved under the `log/` directory.
- ğŸ” **Security Relevance**: Simulates real-world attack scenarios targeting pre-trained model hubs.

## ğŸ“œ Theoretical Motivation

This project is motivated by the increasing reliance on public model hubs and transfer learning in real-world AI applications. While these practices accelerate development, they also introduce a **supply-chain vulnerability**: if the pre-trained encoder is compromised, the integrity of all subsequent uses of that encoder is jeopardized. This is especially concerning for:

- Federated Learning Systems  
- Multi-task and Multi-modal Learning  
- Model-as-a-Service (MaaS) platforms  
- Zero-shot Learning Pipelines  

Our implementation draws insights from state-of-the-art adversarial ML research, including stealthy poisoning, feature-space manipulation, and stealth-preserving optimization.

## âš™ï¸ Installation and Setup

### 1. Clone the Repository

```bash
git clone https://github.com/Miraj-Rahman-AI/Pre-Trained-Backdoor-Attack.git
cd Pre-Trained-Backdoor-Attack
